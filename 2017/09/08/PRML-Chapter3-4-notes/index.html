<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="https://www.pngix.com/pngfile/middle/58-583666_superman-icon-png-logo-superman-transparent-png.png?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="https://www.pngix.com/pngfile/middle/58-583666_superman-icon-png-logo-superman-transparent-png.png?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="Chapter3Chapter1 is about regression and chapter4 is about classification. Although they have some identical form, the way to solve this two kind of problem are different.">
<meta property="og:type" content="article">
<meta property="og:title" content="PRML Chapter3-4 notes">
<meta property="og:url" content="https://lordchao.me/2017/09/08/PRML-Chapter3-4-notes/index.html">
<meta property="og:site_name" content="Chao&#39;s Blog">
<meta property="og:description" content="Chapter3Chapter1 is about regression and chapter4 is about classification. Although they have some identical form, the way to solve this two kind of problem are different.">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-10-10T03:19:43.981Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PRML Chapter3-4 notes">
<meta name="twitter:description" content="Chapter3Chapter1 is about regression and chapter4 is about classification. Although they have some identical form, the way to solve this two kind of problem are different.">
  <link rel="canonical" href="https://lordchao.me/2017/09/08/PRML-Chapter3-4-notes/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>PRML Chapter3-4 notes | Chao's Blog</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chao's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="https://lordchao.me/2017/09/08/PRML-Chapter3-4-notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lordchao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://cdn2.iconfinder.com/data/icons/many-people-flat-icons/128/superman-512.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chao's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">PRML Chapter3-4 notes

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2017-09-08 10:31:33" itemprop="dateCreated datePublished" datetime="2017-09-08T10:31:33+08:00">2017-09-08</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-10-10 11:19:43" itemprop="dateModified" datetime="2019-10-10T11:19:43+08:00">2019-10-10</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Technique/" itemprop="url" rel="index"><span itemprop="name">Technique</span></a></span>

                
                
              
            </span>
          

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2017/09/08/PRML-Chapter3-4-notes/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/09/08/PRML-Chapter3-4-notes/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Chapter3"><a href="#Chapter3" class="headerlink" title="Chapter3"></a>Chapter3</h1><p>Chapter1 is about regression and chapter4 is about classification. Although they have some identical form, the way to solve this two kind of problem are different.</p><a id="more"></a>
<h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>In the term of regression using linear models, we often use a linear combination of fixed nonlinear function $$phi(x)$$ of input variables<br>$$y(\mathbf{w},\mathbf{x}) = w_0 + \sum_{j=1}^{M-1}w_j\phi_j(x)$$<br>where $$w = (w_0,…,w_{M-1})^T$$ and $$\phi = (\phi_0,…,\phi_{M-1})$$. What we want to do in regression problem is to use this function predict the target value, using least square or maximum likelihood we can find a set of coefficients $w$ and then we can make prediction.</p>
<h2 id="Least-square-for-regression"><a href="#Least-square-for-regression" class="headerlink" title="Least square for regression"></a>Least square for regression</h2><p>First let us assume the target variable $t$ has additive Gaussian noise so that<br>$$t = y(\mathbf{w},\mathbf{x}) + \epsilon$$<br>$$p(t|\mathbf{w},\mathbf{x},\beta^{-1}) = \mathcal{N}(t|y(\mathbf{x},\mathbf{w}),\beta^{-1})$$<br>therefore the corresponding likelihood for a data set $$\mathbf{X}={x_1,…,x_n}$$ and target $\mathbf{t}={t_1,…,t_n}$ in the form<br>$$p(\mathbf{t}|\mathbf{X},\mathbf{w},\beta) = \prod^N_{N=1}\mathcal{N}(t_n|w^T\phi(x_n),\beta^{-1})$$<br>taking the logarithm of it<br>$$\ln p(\mathbf{t}|\mathbf{w},\beta) = \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi)-\beta E_D(\mathbf{w})$$<br>where $E_D$ is the sum-of-square erroe funtion is defined by<br>$$E_D(w)=\frac{1}{2}\sum^N_{n=1}(x_n-w^T\phi(x_n))^2$$<br>thus we can see that maximum the likelihood is equivalent to minimizing a sum-of-square error funtion. By taking the derivative of the error function and set it to zero and solve $w$ we can obtain the $w_{ML}$ in least square form.<br>In order to avoid over-fitting, we will add a regularization term to the total error function. The total error becomes:<br>$$\frac{1}{2}\sum^N_{n=1}{t_n-w^\intercal\phi(x)}^2 + \frac{\lambda}{2}w^\intercal w$$</p>
<h2 id="Bayesian-Linear-Regression"><a href="#Bayesian-Linear-Regression" class="headerlink" title="Bayesian Linear Regression"></a>Bayesian Linear Regression</h2><p>To get appropriate model complexity, we can not only use maximum likelihood because this always leads to excessively model complex model. Therefore we need Bayesian treatment which can determine the model complexity using the train data alone instead of using test or validation set.</p>
<p>There are generally two distributions in the Bayesian treatment, the parameter distribution and the predictive distribution. Assuming the precision $\beta$ is known and introducing a prior probability distribution over the model parameter $\mathbf{w}$, the corresponding conjuate prior is given by Gaussian distribution of the form<br>$$p(w) = \mathcal{N}(w|m_0,S_0)$$<br>the posterior distribution takes form<br>$$p(w|t) = \mathcal{N}(w|m_N,S_N)$$<br>where<br>$$m_{N} = S_{N}(S_0^{-1}m_0+\beta \Phi^\intercal t)$$<br>$$S_N^{-1} = S_0^{-1} + \beta \Phi^\intercal \Phi$$<br>we can see that both the expression of the mean and the precision have two parts, one can be seen as the prior and the other can be seen as the evidence after we get data points. The expression in identical to the posterior distribution with univariate Gaussian prior and known precision<br>$$\mu_N = \frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0 + \frac{N\sigma^2}{N\sigma_0^2+\sigma^2}\mu_{ML}$$<br>$$\frac{1}{\sigma^2_N} = \frac{1}{\sigma_0^2}+\frac{N}{\sigma_0^2}$$<br>$$\mu_{ML}=\frac{1}{N}\sum^N_{n=1}x_n-w$$<br>The log of posterior discussion is given by the sum of the log likelihood and the log of the prior and, as a function of $w$, takes form<br>$$\ln p(w|t)=-\frac{\beta}{2}\sum_{n=1}^N{t_n-w^\intercal\phi(x_n)}^2-\frac{\alpha}{2}w^\intercal w+const$$<br>maximum of this posterior discussion with respect to $w$ is therefore equivalent to minimize the sum-of-square error function with addition of a quadratic regularization term, corresponding to the previous regularized form we can see that $\lambda=\alpha/\beta$<br>Having the parameter discussion we can then make prediction through predictive distribution defined by<br>$$p(t|\mathbf{t})=\int p(t|\mathbf{w},\beta)p(w|\mathbf{t},\alpha,\beta)d\mathbf{w}$$<br>we can see that involves the convolution of two Gaussian discussions.<br>It takes form<br>$$p(t|x,\mathbf{t},\alpha,\beta)=\mathcal{N}(t|m_N^\intercal\phi(x),\sigma^2(x))$$<br>the variance<br>$$\sigma^2_N(x)=\frac{1}{\beta}+\phi(x)^\intercal\phi(x)$$<br>第一项代表了数据本身所带来的噪声，这一项是不可改变的，同时也限制了预测所能达到的最高准确率，第二项代表了预测$w$的不确定性，这一项会随着$N$的变大而逐渐变小，也就是随着数据量越大，我们的预测也就越准确，从而方差也就会越来越小。</p>
<h2 id="Bayesian-Model-Comparison"><a href="#Bayesian-Model-Comparison" class="headerlink" title="Bayesian Model Comparison"></a>Bayesian Model Comparison</h2><p>In the previous section, we introduce the cross validation and model average to avoid over-fitting， but both of these method can not work only with training data. To do this, to approximate the model average, we shall use a single probable model alone to make prediction. It takes form<br>$$p(D)=\int p(D|w)p(w)\simeq p(D|w_{map})\frac{\Delta w_{posterior}}{\Delta_{prior}}$$<br>takeing the logarithm<br>$$\ln p(D)=\ln p(D|w_{MAP})+\ln (\frac{\Delta w_{posterior}}{\Delta_{prior}})$$<br>The first term represent the fit to the data given by the most probable parameter value. The second term penalizes the model according to its complexity. If $\Delta w_{posterior}&lt;\Delta w_{prior}$ the second will be negative.</p>
<h2 id="Evidence-Approximation"><a href="#Evidence-Approximation" class="headerlink" title="Evidence Approximation"></a>Evidence Approximation</h2><p>I was also little confused about this section, there are some question about it, so I just write down the though for this moment and I shall revise the part later.</p>
<p>If we introduce a full Bayesian treatment of linear basis function model, we need the prior distribution over hyperparameters $\alpha$ and $\beta$, therefore the corresponding predictive distribution is given by<br>$$p(t|\mathbf{t})=\int\int\int p(t|w,\beta)p(w|\mathbf{t},\alpha,\beta)p(\alpha,\beta|\mathbf{t})dwd\alpha d\beta$$<br>the posterior discussion for $\alpha$ $\beta$ is given by<br>$$p(\alpha,\beta)\propto p(\mathbf{t}|\alpha,\beta)p(\alpha,\beta)$$<br>the marginal likelihood or the evidence function is therefore $p(\mathbf{t}|\alpha,\beta)=\int p(\mathbf{t}|w,\beta)p(w|\alpha)dw$<br>If we do the evaluation of this function we can get<br>$$\ln p(\mathbf(t)|\alpha,\beta)=\frac{M}{2}\ln\alpha+\frac{M}{2}\ln\beta-E(m_N)-\frac{M}{2}\ln|A|-\frac{M}{2}\ln(2\pi)$$<br>where $m_N=\beta^{-1}A\Phi^\intercal \mathbf{t}$ and $A$ is the Hessian matrix of the error function.<br>to maximum this function, we need to find its stationary points. Specially, we first take the derivative of it with respect $alpha$, and then take derivative of it with respect $\beta$, then we will get two implicit expression. To solve $\alpha$ and $\beta$ we can first initial a value and then using the expression to calculate $m_N$ and $\gamma$ and then re-estimate $\alpha$ and $\beta$.(也就是我们先给定一个它一个初始值，通过这个初始值又会计算出一个它的值，再用新得到继续重复计算) The expression of $\alpha$ and $\beta$ is<br>$$\alpha=\frac{\gamma}{m_N^\intercal m_N}$$</p>
<p>$$\frac{1}{\beta}=\frac{1}{N-\gamma}\sum_{n=1}^N{t_n-m_N\phi(x_n)}^2$$<br>$$\gamma=\sum\limits_{\substack{i}}\frac{\lambda}{\alpha+<br>  \lambda}$$<br>  $\gamma$ measure the effective total number of well determined parameter.</p>
<h2 id="Effective-number"><a href="#Effective-number" class="headerlink" title="Effective number"></a>Effective number</h2><p>  时间关系，这一部分先用中文了。从这一部分，我深刻的体会到，贝叶斯方法和极大似然最大的不同就是先验先验先验（重要的事情说三遍）。由于有先验概率的存在，使得贝叶斯方法与极大似然估计得出结果有会有一些不同。就均值来说，贝叶斯与极大似然估计会得出一样的结果，这也是合乎逻辑的，均值可以看作是最后的预测结果，如果给一个线性模型足够多的项，它一定会得出一个无偏的结果。但是极大似然估计出的方差就会天然的偏小。这是因为在极大似然的过程中它没有使用真实均值，而是使用了均值的一个采样，如果将使用不同数据得出的方差进行平均，得出的结果就会比单一的极大似然估计大（也就是极大似然会倾向于方差小而偏差也小的模型）。造成着一些的原因都是因为我们的数据中存在噪声，也就是说，之前一部分得出的 $\gamma$ 决定了哪一些模型是被数据所紧密限制的，剩余的$M-\gamma$个参数也就是与先验有关，在先验中设定的了。线面就给出极大似然和贝叶斯对方差的估计结果<br>  $$\sigma^2_{ML}=\frac{1}{N}\sum(x_n-\mu_{ML})^2$$<br>  $$\sigma^2_{ML}=\frac{1}{N-\gamma}\sum$(x_n-\mu_{ML})^2$$<br>  我们可以看出贝叶斯的结果比极大似然会大因为分母上间去了 $\gamma$，也就是说我们考虑了先验。</p>
<h1 id="Charpter4"><a href="#Charpter4" class="headerlink" title="Charpter4"></a>Charpter4</h1><p>This chapter related to Laplace approximation and the concept of discriminative model and generative model. These are some advance topic and will be introduce in detail in later chapter.</p>
<p>Differ from the regression problem, the model in classification problem are no longer linear in the parameters due to the nonlinear function $f(.)$</p>
<p>In Chapter1, we identified three distinct approaches to do classification, the simplest is to construct a discriminant function that directly assign each vector x to a specific class. The other is to models a conditional distribution $p(C_k|x)$ in an <em>inference</em> stage and then subsequently using this distribution to make decision.</p>
<p>In discriminant functions, this book briefly introduced the Fisher’s linear discriminant function and least squares for classification and we can view the Fisher’s linear function as a special case of least square solution with change the target value. As well as the perceptron algorithm.</p>
<h2 id="Maximum-likelihood-solution"><a href="#Maximum-likelihood-solution" class="headerlink" title="Maximum likelihood solution"></a>Maximum likelihood solution</h2><p>As usual we can first write down the likelihood function and then take its logarithm form and then find its maximum. In this case, for a data point $x_n$ from class $C_1$, we have $t_n=1$ and hence we can define the joint probability<br>$$p(x_n,C_1)=p(C_1)p(x_n|C_1)=\pi\mathcal{N}(x_n|\mu_1,\Sigma)$$<br>where $pi$ denote the prior class probability $p(C_1)$. Thus the likelihood function is given by<br>$$p(\mathbf{t},\mathbf{X})=\prod_{n=1}^N[\pi\mathcal{N(x_n|\mu_1,\Sigma)}]^{t_n}[（1-\pi）\mathcal{N(x_n|\mu_2,\Sigma)}]^{1-t_n}$$<br>the solution for maximum likelihood of prior $\pi$ is the fraction of total number of class $C_1$, $\pi=\frac{1}{N}\sum_{n=1}^Nt_n$. And the maximum likelihood solution for the mean $\mu$ is the mean of all the input vectors $x_n$ assigned to class $C_1$. $\mu_1=\frac{1}{N_1}\sum_{n=1}^Nt_nx_n$,$\mu_2=\frac{1}{N_2}\sum_{n=1}^N(1-t_n)x_n$</p>
<h2 id="Laplace-Approximation"><a href="#Laplace-Approximation" class="headerlink" title="Laplace Approximation"></a>Laplace Approximation</h2><p><strong>Problem and motivation</strong>: We cannot integrate over the parameter vector $\mathcal{w}$ since the posterior distribution is no longer  Gaussian(Why?). It is therefore necessary to introduce some form of approximation. Here comes to the Laplace approximation - a simply but widely used framework.<br><strong>Main idea</strong>: Find a Gaussian approximation $q(x)$ which is centred on a mode of the distribution $p(z)$. $p(x)=\frac{1}{Z}f(x)$, $z=\int f(z)$. Then we will use a two order Taylor expansion of $\ln f(z)$ centred on the mode $z_0$ due to the properties of Gaussian distribution that its logarithm is quadratic function.<br><strong>Process:</strong><br>First step is to find a a point $z_0$ such that $p^\prime(z_0)=0$<br>then use Taylor expansion of $\ln f(z)$<br>$$\ln f(z)\simeq\ln(z_0)-\frac{1}{2}A(z-z_0)^2$$<br>where<br>$$A=-\frac{d^2}{dz^2}\ln f(z)\Big{|}_{z=z_0}$$<br>the first term gonna disappear so<br>$$f(z)\simeq f(z_0)\exp{-\frac{A}{2}(z-z_0)}^2$$<br>then we can construct the corresponding Gaussian distribution<br>$$q(z)=(\frac{A}{2})^{\frac{1}{2}}\exp{\frac{A}{2}(z-z_0)^2}$$<br>Note that the Gaussian approximation will only be well defined if its precision $A&gt;0$, in other words the stationary point $z_0$ must be a local maximum, so that the second derivative of $f(z)$ at the point $z_0$ is negative.<br><strong>Limitation:</strong> When the number of data points is relatively large, the Laplace approximation will be useful due to the central limit theorem. The major limitation of Laplace approximation is that it based purely on the aspect of the true distribution at a <strong>specific</strong> value of the variable, so can fail to capture important global properties.</p>
<h2 id="Bayesian-Logistic-Regression"><a href="#Bayesian-Logistic-Regression" class="headerlink" title="Bayesian Logistic Regression"></a>Bayesian Logistic Regression</h2><p>The key of Bayesian treatment is to find a predictive function by  marginalizing with respect to the posterior distribution $p(w|t)$, which is itself approximated by $q(x)$ to make prediction, so that<br>$$p(C_1|\phi,\mathbf{t})=\int p(w|\mathbf{t})p(C_1|\phi,w)dw\simeq \int \sigma(w^\intercal\phi)q(w)dw$$</p>
<p>Using the marginal Gaussian equation we can get a result of predictive function that cannot be evaluated. Therefore we can approximate the sigmoid function using probit function for their similarity.</p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2017/08/10/记一次完整的翻墙经历/" rel="next" title="记一次完整的翻墙经历">
                  <i class="fa fa-chevron-left"></i> 记一次完整的翻墙经历
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2017/09/08/PRML-Chapter1-2-notes/" rel="prev" title="PRML Chapter1-2 notes">
                  PRML Chapter1-2 notes <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter3"><span class="nav-number">1.</span> <span class="nav-text">Chapter3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Regression"><span class="nav-number">1.1.</span> <span class="nav-text">Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Least-square-for-regression"><span class="nav-number">1.2.</span> <span class="nav-text">Least square for regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayesian-Linear-Regression"><span class="nav-number">1.3.</span> <span class="nav-text">Bayesian Linear Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayesian-Model-Comparison"><span class="nav-number">1.4.</span> <span class="nav-text">Bayesian Model Comparison</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evidence-Approximation"><span class="nav-number">1.5.</span> <span class="nav-text">Evidence Approximation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Effective-number"><span class="nav-number">1.6.</span> <span class="nav-text">Effective number</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Charpter4"><span class="nav-number">2.</span> <span class="nav-text">Charpter4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Maximum-likelihood-solution"><span class="nav-number">2.1.</span> <span class="nav-text">Maximum likelihood solution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Laplace-Approximation"><span class="nav-number">2.2.</span> <span class="nav-text">Laplace Approximation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayesian-Logistic-Regression"><span class="nav-number">2.3.</span> <span class="nav-text">Bayesian Logistic Regression</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="https://cdn2.iconfinder.com/data/icons/many-people-flat-icons/128/superman-512.png"
      alt="lordchao">
  <p class="site-author-name" itemprop="name">lordchao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
        
      </div>
    
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lordchao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.1</div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>
<script src="/js/next-boot.js?v=7.4.1"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        extensions: ['[mhchem]/mhchem.js'],
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://lordchao.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  function disqus_config() {
    this.page.url = "https://lordchao.me/2017/09/08/PRML-Chapter3-4-notes/";
    this.page.identifier = "2017/09/08/PRML-Chapter3-4-notes/";
    this.page.title = 'PRML Chapter3-4 notes';};
  function loadComments() {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://lordchao.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  }
    window.addEventListener('load', loadComments, false);
  
</script>

</body>
</html>
